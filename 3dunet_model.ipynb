{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58095648-fc01-4223-87f9-d49725a6c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from scipy.ndimage import zoom\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1620ec7b-6db2-4adc-a3d5-b0c2cd707bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1\n",
    "'''\n",
    "Initializes the dataset for 3D U-Net\n",
    "modalities: Tuple of 4 MRI types\n",
    "resize_hw: Only height & width (128×128), depth (182) is untouched\n",
    "only_tumor=True: if True, only includes patients with tumor (seg > 0)'''\n",
    "\n",
    "class BraTS3DDataset(Dataset):\n",
    "    def __init__(self, data_dir, patients=None, modalities=(\"t1n\", \"t1c\", \"t2w\", \"t2f\"),\n",
    "                 resize_hw=(128, 128), only_tumor=True):\n",
    "        self.modalities = modalities\n",
    "        self.resize_hw = resize_hw  # Only resize H, W\n",
    "        self.only_tumor = only_tumor\n",
    "        self.data_dir = data_dir\n",
    "        self.patients = sorted(os.listdir(self.data_dir)) if patients is None else patients\n",
    "\n",
    "        '''Filters 1350 patients down to a smaller subset containing at least one labeled region.\n",
    "         Why this matters:\n",
    "            Ensures training only includes positive examples (to counter class imbalance).\n",
    "            Especially useful for 3D U-Net which is memory heavy → better to train on informative samples.'''\n",
    "        self.filtered = []\n",
    "        for pat in self.patients:\n",
    "            seg_path = os.path.join(self.data_dir, pat, f\"{pat}-seg.nii.gz\")\n",
    "            seg = nib.load(seg_path).get_fdata()\n",
    "            if self.only_tumor:\n",
    "                if np.any(seg > 0):\n",
    "                    self.filtered.append(pat)\n",
    "            else:\n",
    "                self.filtered.append(pat)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered) # Returns the number of patients (each sample = 1 full 3D volume)\n",
    "\n",
    "    # Load & Preprocess 3D Volume\n",
    "    def __getitem__(self, idx):\n",
    "        pat = self.filtered[idx]\n",
    "        folder = os.path.join(self.data_dir, pat)\n",
    "        H, W = self.resize_hw # Resizes every modality slice to 128×128\n",
    "        vols = []\n",
    "\n",
    "        # Load Each Modality → Normalize → Resize\n",
    "        for mod in self.modalities:\n",
    "            img = nib.load(os.path.join(folder, f\"{pat}-{mod}.nii.gz\")).get_fdata().astype(np.float32)\n",
    "            mask = img != 0\n",
    "            mu, sigma = (img[mask].mean(), img[mask].std()) if mask.sum() > 0 else (img.mean(), img.std())\n",
    "            img = (img - mu) / (sigma + 1e-8)\n",
    "\n",
    "            # Resize H and W only → shape: (D, H, W)\n",
    "            zoom_factors = (1.0, H / img.shape[1], W / img.shape[2])\n",
    "            img_resized = zoom(img, zoom_factors, order=3) # order=3: bicubic interpolation for smooth results\n",
    "            vols.append(img_resized)\n",
    "\n",
    "        x = np.stack(vols, axis=0)  # shape: (4, D, H, W). Stacks all 4 modalities → final shape (4, 182, 128, 128)\n",
    "\n",
    "        seg = nib.load(os.path.join(folder, f\"{pat}-seg.nii.gz\")).get_fdata().astype(np.int64) # Loads 3D segmentation mask\n",
    "        seg_resized = zoom(seg, zoom_factors, order=0) # Resizes using nearest neighbor (order=0) to preserve label integrity\n",
    "        y = seg_resized  # shape: (D, H, W)\n",
    "\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).long()\n",
    "        # x: Tensor of shape (4, 182, 128, 128) → full 3D input\n",
    "        # y: Tensor of shape (182, 128, 128) → label map\n",
    "\n",
    "\n",
    "data_dir = \"/scratch/scai/mtech/aib232081/brain_tumor_detection/BraTS2024_dataset/BraTS2024-BraTS-GLI-TrainingData/training_data1_v2\"\n",
    "all_patients = sorted(os.listdir(data_dir))  # 1350 patients\n",
    "\n",
    "# Split (80/10/10) Train: ~1080, Val: ~135, Test: ~135\n",
    "train_val, test = train_test_split(all_patients, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dataset\n",
    "train_ds = BraTS3DDataset(data_dir, patients=train, only_tumor=True)\n",
    "val_ds   = BraTS3DDataset(data_dir, patients=val, only_tumor=True)\n",
    "test_ds  = BraTS3DDataset(data_dir, patients=test, only_tumor=True)\n",
    "\n",
    "# DataLoader with full prefetch optimizations\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=8,\n",
    "                          pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
    "\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=4,\n",
    "                          pin_memory=True, persistent_workers=True)\n",
    "\n",
    "test_loader  = DataLoader(test_ds, batch_size=2, shuffle=False, num_workers=4,\n",
    "                          pin_memory=True, persistent_workers=True)\n",
    "\n",
    "'''\n",
    "Small batch_size=4 (3D tensors consume a lot of GPU memory)\n",
    "num_workers=8: Use 8 CPU threads to load data in parallel\n",
    "persistent_workers=True: avoids re-spawning workers every epoch\n",
    "prefetch_factor=4: each worker loads 4 batches in advance → keeps GPU busy'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e64b4-b95d-41da-93a4-1975ebb7c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2\n",
    "class ConvBlock3D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, p=0.3):  # dropout probability\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(p),\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        ''' Two 3D convolution layers (kernel=3, padding=1 keeps dimensions same)\n",
    "            Each followed by:\n",
    "            BatchNorm3d for stable training\n",
    "            ReLU activation\n",
    "            Dropout3d(p) in the middle → helps regularize and prevent overfitting in deep 3D networks'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "# in_ch=4: input channels = 4 modalities (t1n, t1c, t2w, t2f)\n",
    "# base_ch=64: number of channels at the first layer, scaled up in deeper layers\n",
    "# n_classes=5: output segmentation labels (0–4)\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_ch=4, base_ch=64, n_classes=5):  # base_ch can be 32 or 64\n",
    "        super().__init__()\n",
    "        self.enc1 = ConvBlock3D(in_ch, base_ch) # Input: [B, 4, 182, 128, 128] → [B, 64, 182, 128, 128]\n",
    "        self.pool1 = nn.MaxPool3d(2) # spatial depth halves at each step. Downsample by factor of 2 → [B, 64, 91, 64, 64]; 182 → 91 (depth), 128 → 64 (height/width)\n",
    "\n",
    "        self.enc2 = ConvBlock3D(base_ch, base_ch * 2) # 64 → 128 channels\n",
    "        self.pool2 = nn.MaxPool3d(2) # 91 → 45 (depth), 64 → 32 (height/width)\n",
    "\n",
    "        self.enc3 = ConvBlock3D(base_ch * 2, base_ch * 4) # 128 → 256 channels\n",
    "        self.pool3 = nn.MaxPool3d(2) # 45 → 22 (depth), 32 → 16 (height/width)\n",
    "\n",
    "        self.bottleneck = ConvBlock3D(base_ch * 4, base_ch * 8) # Deepest layer, 256 → 512 channels; Learns global features — structure, tumor shape, context\n",
    "\n",
    "        # Decoder Path (Upsampling + Skip Connections)\n",
    "        self.dec3 = ConvBlock3D(base_ch * 12, base_ch * 4)\n",
    "        '''\n",
    "        Takes:\n",
    "            Upsampled bottleneck (512 → 256 channels),\n",
    "            Concatenated with encoder3 output (256 channels),\n",
    "            Total input: 512 → so channels = base_ch * 12 = 768\n",
    "            Actually: 512 + 256 = 768, but this line assumes base_ch * 12, which is correct if base = 64 (64×12=768)'''\n",
    "        self.dec2 = ConvBlock3D(base_ch * 6, base_ch * 2) # same as above: Interpolate → concatenate with skip connection → reduce channels\n",
    "        self.dec1 = ConvBlock3D(base_ch * 3, base_ch)\n",
    "        # Dimensions: recover to 128×128×182\n",
    "\n",
    "        self.out_conv = nn.Conv3d(base_ch, n_classes, kernel_size=1) # Final prediction layer: 64 → 5 classes using 1×1×1 conv. [B, 5, 182, 128, 128] → raw logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder forward pass with downsampling. Stores e1, e2, e3 for skip connections\n",
    "        e1 = self.enc1(x)                 # -> [B, C, D, H, W]\n",
    "        e2 = self.enc2(self.pool1(e1))   # -> [B, 2C, D/2, H/2, W/2]\n",
    "        e3 = self.enc3(self.pool2(e2))   # -> [B, 4C, D/4, H/4, W/4]\n",
    "        b = self.bottleneck(self.pool3(e3))  # -> [B, 8C, D/8, H/8, W/8]\n",
    "\n",
    "        u3 = F.interpolate(b, size=e3.shape[2:], mode='trilinear', align_corners=False) # Interpolates bottleneck back to e3’s shape\n",
    "\n",
    "        d3 = self.dec3(torch.cat([u3, e3], dim=1)) # Concatenate and pass through decoder block. Same pattern follows for u2, d2, then u1, d1.\n",
    "\n",
    "        u2 = F.interpolate(d3, size=e2.shape[2:], mode='trilinear', align_corners=False)\n",
    "        d2 = self.dec2(torch.cat([u2, e2], dim=1))\n",
    "\n",
    "        u1 = F.interpolate(d2, size=e1.shape[2:], mode='trilinear', align_corners=False)\n",
    "        d1 = self.dec1(torch.cat([u1, e1], dim=1))\n",
    "\n",
    "        return self.out_conv(d1) # Final logits (no softmax — handled in loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba81fb-8372-4642-a142-1bbe98308238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3\n",
    "\n",
    "def soft_dice_loss_3d(pred, target, epsilon=1e-6):\n",
    "    \"\"\"Soft dice loss for 3D volumetric data\"\"\"\n",
    "    pred = F.softmax(pred, dim=1) # Applies softmax over class dimension → shape becomes [B, C, D, H, W]\n",
    "        # Each voxel now contains class probability distribution\n",
    "\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 4, 1, 2, 3).float().to(pred.device)\n",
    "        # Converts ground truth target of shape [B, D, H, W] to one-hot format → [B, C, D, H, W]\n",
    "        # Matches shape with prediction for per-class Dice calculation\n",
    "\n",
    "    intersect = (pred * target_onehot).sum(dim=(2, 3, 4)) # true positives per class per batch\n",
    "    denominator = pred.sum(dim=(2, 3, 4)) + target_onehot.sum(dim=(2, 3, 4)) # predicted + actual voxels (union)\n",
    "\n",
    "    dice = (2. * intersect + epsilon) / (denominator + epsilon) # Computes average Dice across all classes and batch\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "def combined_3d_loss(pred, target, alpha=0.3, beta=0.7):\n",
    "    # Class weights: reduce emphasis on background\n",
    "    weights = torch.tensor([0.01, 1.0, 1.0, 1.0, 1.0], device=pred.device)\n",
    "        # Background (label 0) gets very low weight = 0.01\n",
    "        # Tumor classes (labels 1–4) get full weight = 1.0\n",
    "\n",
    "    ce = F.cross_entropy(pred, target, weight=weights, reduction='mean') # Standard weighted cross-entropy over [B, C, D, H, W] logits and [B, D, H, W] targets\n",
    "    dice = soft_dice_loss_3d(pred, target)\n",
    "\n",
    "    total = alpha * ce + beta * dice # Final loss = 30% CE + 70% Dice\n",
    "    return total, ce.item(), dice.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c077a-da8d-4bd5-bb87-6cb7729a328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4\n",
    "\n",
    "import time\n",
    "\n",
    "# === Metric Calculation ===\n",
    "def compute_metrics_3d(pred, target, num_classes=5): # Computes per-class Dice and IoU for 3D predictions.\n",
    "    pred_soft = F.softmax(pred, dim=1) # Converts raw logits to predicted class labels using argmax\n",
    "    pred_labels = pred_soft.argmax(dim=1)\n",
    "\n",
    "    one_hot_pred = F.one_hot(pred_labels, num_classes).permute(0, 4, 1, 2, 3).float() # One-hot encodes to shape [B, C, D, H, W]\n",
    "    one_hot_target = F.one_hot(target, num_classes).permute(0, 4, 1, 2, 3).float()\n",
    "\n",
    "    one_hot_pred = one_hot_pred.to(pred.device)\n",
    "    one_hot_target = one_hot_target.to(pred.device)\n",
    "\n",
    "    # Computes per-class Dice and IoU scores\n",
    "    intersect = (one_hot_pred * one_hot_target).sum(dim=(2, 3, 4))\n",
    "    union = one_hot_pred.sum(dim=(2, 3, 4)) + one_hot_target.sum(dim=(2, 3, 4)) - intersect\n",
    "    dice = (2 * intersect) / (one_hot_pred.sum(dim=(2, 3, 4)) + one_hot_target.sum(dim=(2, 3, 4)) + 1e-6)\n",
    "    iou = intersect / (union + 1e-6)\n",
    "\n",
    "    return dice.mean(dim=0).cpu().numpy(), iou.mean(dim=0).cpu().numpy()\n",
    "        # Returns per-class average Dice and IoU (across the batch)\n",
    "\n",
    "# === Model, Optimizer, and Scaler Setup ===\n",
    "model = UNet3D(in_ch=4, base_ch=64, n_classes=5).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "scaler = GradScaler() # GradScaler() for automatic mixed precision training (saves memory and speeds up)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2) \n",
    "    # ReduceLROnPlateau: halves learning rate if validation loss doesn’t improve for 2 epochs\n",
    "\n",
    "# === Logging ===\n",
    "log_path = \"training_logs_3d_unet_trial24b.csv\"\n",
    "checkpoint_path = \"checkpoint_3d_unet_trial24b.pth\"\n",
    "history = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs = 50\n",
    "\n",
    "# Training phase:\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_ce, total_dice = 0, 0\n",
    "    dice_train, iou_train = [], []\n",
    "\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Train\"): # load a batch of 3D input and labels\n",
    "        xb, yb = xb.cuda(), yb.cuda() # Shapes: xb → [B, 4, D, H, W], yb → [B, D, H, W]\n",
    "\n",
    "        with autocast(): # Runs the model in mixed precision using autocast\n",
    "            pred = model(xb)\n",
    "            loss, ce_val, dice_val = combined_3d_loss(pred, yb) # Computes combined loss: 0.3 × CE + 0.7 × Dice\n",
    "\n",
    "        optimizer.zero_grad() # Standard AMP training block with scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_ce += ce_val\n",
    "        total_dice += dice_val\n",
    "\n",
    "        d, i = compute_metrics_3d(pred, yb) # Computes and logs per-class metrics for each batch\n",
    "        dice_train.append(d)\n",
    "        iou_train.append(i)\n",
    "\n",
    "    dice_train = np.array(dice_train)\n",
    "    iou_train = np.array(iou_train)\n",
    "\n",
    "    # Validation phase:\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_ce, val_dice = 0, 0\n",
    "    dice_val, iou_val = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Val\"):\n",
    "            xb, yb = xb.cuda(), yb.cuda()\n",
    "\n",
    "            with autocast():\n",
    "                pred = model(xb)\n",
    "                loss, ce_val, dice_val_ = combined_3d_loss(pred, yb)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            val_ce += ce_val\n",
    "            val_dice += dice_val_\n",
    "\n",
    "            d, i = compute_metrics_3d(pred, yb)\n",
    "            dice_val.append(d)\n",
    "            iou_val.append(i)\n",
    "\n",
    "    dice_val = np.array(dice_val)\n",
    "    iou_val = np.array(iou_val)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    mean_dice_train = dice_train.mean()\n",
    "    mean_dice_val = dice_val.mean()\n",
    "    mean_iou_train = iou_train.mean()\n",
    "    mean_iou_val = iou_val.mean()\n",
    "\n",
    "    scheduler.step(total_val_loss)\n",
    "\n",
    "    if total_val_loss < best_val_loss:\n",
    "        best_val_loss = total_val_loss\n",
    "        torch.save(model.state_dict(), \"best_3d_unet_model_trial24b.pth\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': total_train_loss,\n",
    "        'val_loss': total_val_loss,\n",
    "        'mean_dice_train': mean_dice_train,\n",
    "        'mean_dice_val': mean_dice_val,\n",
    "        'mean_iou_train': mean_iou_train,\n",
    "        'mean_iou_val': mean_iou_val,\n",
    "        'dice_ET_train': dice_train[:,1].mean(),\n",
    "        'dice_NETC_train': dice_train[:,2].mean(),\n",
    "        'dice_SNFH_train': dice_train[:,3].mean(),\n",
    "        'dice_RC_train': dice_train[:,4].mean(),\n",
    "        'dice_ET_val': dice_val[:,1].mean(),\n",
    "        'dice_NETC_val': dice_val[:,2].mean(),\n",
    "        'dice_SNFH_val': dice_val[:,3].mean(),\n",
    "        'dice_RC_val': dice_val[:,4].mean(),\n",
    "        'epoch_time_sec': epoch_time,\n",
    "        'lr': lr,\n",
    "        'train_ce': total_ce / len(train_loader),\n",
    "        'train_dice': total_dice / len(train_loader),\n",
    "        'val_ce': val_ce / len(val_loader),\n",
    "        'val_dice': val_dice / len(val_loader),\n",
    "    })\n",
    "\n",
    "    pd.DataFrame(history).to_csv(log_path, index=False)\n",
    "    print(f\"Epoch {epoch+1} logged. Dice: Train={mean_dice_train:.4f}, Val={mean_dice_val:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee74480-2f5a-46c9-82ea-3cf0c8ef410a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lidar_env)",
   "language": "python",
   "name": "lidar_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
